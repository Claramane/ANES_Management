"""
è³‡æ–™åŒæ­¥å·¥ä½œå™¨æ¨¡çµ„

è² è²¬åŸ·è¡Œè³‡æ–™åº«åŒæ­¥é‚è¼¯ï¼ŒåŒ…æ‹¬å¢é‡åŒæ­¥ã€UPSERT æ“ä½œå’Œç‹€æ…‹ç®¡ç†
"""

import asyncio
import asyncpg
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
from config import Config

logger = logging.getLogger(__name__)


# ============================================================================
# å¸¸æ•¸å®šç¾©
# ============================================================================

# è¡¨æ ¼åŒæ­¥é †åº (ä¾ç…§å¤–éµä¾è³´é—œä¿‚)
TABLE_SYNC_ORDER = [
    'users',                        # åŸºç¤è¡¨æ ¼ï¼ˆç„¡ä¾è³´ï¼‰
    'formula_schedules',            # ä¾è³´ users
    'monthly_schedules',            # ä¾è³´ users
    'schedule_versions',            # ä¾è³´ users
    'schedule_version_diffs',       # ä¾è³´ schedule_versions
    'schedule_changes',             # ä¾è³´ schedule_versions
    'shift_swap_requests',          # ä¾è³´ users, schedules
    'overtime_records',             # ä¾è³´ users, schedules
    'overtime_points',              # ä¾è³´ users
    'overtime_monthly_scores',      # ä¾è³´ users
    'overtime_summaries',           # ä¾è³´ users
    'announcements',                # ä¾è³´ users
    'doctor_schedules',             # ç¨ç«‹è¡¨æ ¼
    'day_shift_doctors',            # ä¾è³´ doctor_schedules
    'doctor_schedule_update_logs',  # ä¾è³´ doctor_schedules
    # 'webauthn_credentials',       # è·³éï¼šåŒ…å«æ•æ„Ÿç”Ÿç‰©è¾¨è­˜è³‡æ–™ï¼Œä¸”æ¸¬è©¦ç’°å¢ƒä¸éœ€è¦
    'logs',                         # æœ€å¾Œï¼ˆç„¡ä¾è³´ï¼‰
]

# æ•æ„Ÿæ¬„ä½ï¼ˆä¸åŒæ­¥ï¼‰
SENSITIVE_FIELDS = {
    'users': ['password'],
    # 'webauthn_credentials': ['credential_id', 'public_key']  # æš«æ™‚è¨»è§£ï¼Œå› ç‚ºé€™äº›æ¬„ä½æ˜¯ NOT NULL
}


# ============================================================================
# åŒæ­¥ç‹€æ…‹ç®¡ç†
# ============================================================================

class SyncStateManager:
    """åŒæ­¥ç‹€æ…‹ç®¡ç†å™¨"""

    def __init__(self, state_file: str = 'sync_state.json'):
        self.state_file = Path(state_file)
        self.state = self._load_state()

    def _load_state(self) -> Dict:
        """è¼‰å…¥åŒæ­¥ç‹€æ…‹"""
        if self.state_file.exists():
            try:
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"ç„¡æ³•è¼‰å…¥åŒæ­¥ç‹€æ…‹æª”æ¡ˆ: {e}")
        return {}

    def save_state(self):
        """å„²å­˜åŒæ­¥ç‹€æ…‹"""
        try:
            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(self.state, f, indent=2, ensure_ascii=False, default=str)
        except Exception as e:
            logger.error(f"ç„¡æ³•å„²å­˜åŒæ­¥ç‹€æ…‹: {e}")

    def get_last_sync_time(self, table: str) -> Optional[datetime]:
        """å–å¾—è¡¨æ ¼çš„æœ€å¾ŒåŒæ­¥æ™‚é–“"""
        if table in self.state:
            try:
                return datetime.fromisoformat(self.state[table]['last_sync_time'])
            except (KeyError, ValueError) as e:
                logger.warning(f"ç„¡æ³•è§£æ {table} çš„åŒæ­¥æ™‚é–“: {e}")
        return None

    def update_sync_time(self, table: str, sync_time: datetime, records_synced: int):
        """æ›´æ–°è¡¨æ ¼çš„åŒæ­¥æ™‚é–“"""
        self.state[table] = {
            'last_sync_time': sync_time.isoformat(),
            'records_synced': records_synced,
            'updated_at': datetime.now().isoformat()
        }
        self.save_state()


# ============================================================================
# åŒæ­¥å·¥ä½œå™¨
# ============================================================================

class SyncWorker:
    """è³‡æ–™åŒæ­¥å·¥ä½œå™¨"""

    def __init__(self, config: Config):
        self.config = config
        self.state_manager = SyncStateManager()

    async def sync_all_targets(self):
        """åŒæ­¥åˆ°æ‰€æœ‰ç›®æ¨™ç’°å¢ƒ"""
        logger.info("=" * 60)
        logger.info("é–‹å§‹åŸ·è¡Œè³‡æ–™åº«åŒæ­¥")
        logger.info(f"åŒæ­¥ç›®æ¨™: {self.config.SYNC_TARGETS}")
        logger.info("=" * 60)

        total_synced = 0
        sync_start_time = datetime.now()

        for target_name, target_url in self.config.target_urls.items():
            try:
                logger.info(f"\nğŸ“Š åŒæ­¥åˆ°ç›®æ¨™ç’°å¢ƒ: {target_name}")
                count = await self.sync_to_target(target_name, target_url)
                total_synced += count
                logger.info(f"âœ… ç›®æ¨™ {target_name} åŒæ­¥å®Œæˆ: {count} ç­†è¨˜éŒ„")
            except Exception as e:
                logger.error(f"âŒ åŒæ­¥åˆ° {target_name} å¤±æ•—: {e}", exc_info=True)

        sync_duration = (datetime.now() - sync_start_time).total_seconds()
        logger.info("=" * 60)
        logger.info(f"åŒæ­¥å®Œæˆ: ç¸½å…± {total_synced} ç­†è¨˜éŒ„ï¼Œè€—æ™‚ {sync_duration:.2f} ç§’")
        logger.info("=" * 60)

    async def sync_to_target(self, target_name: str, target_url: str) -> int:
        """åŒæ­¥åˆ°å–®ä¸€ç›®æ¨™ç’°å¢ƒ"""
        total_records = 0

        # é€£æ¥ä¾†æºå’Œç›®æ¨™è³‡æ–™åº«
        source_conn = await asyncpg.connect(self.config.SOURCE_DB_URL)
        target_conn = await asyncpg.connect(target_url)

        try:
            # ä¾åºåŒæ­¥æ¯å€‹è¡¨æ ¼
            for table in TABLE_SYNC_ORDER:
                try:
                    count = await self.sync_table(table, source_conn, target_conn)
                    total_records += count

                    if count > 0:
                        logger.info(f"  âœ“ {table}: {count} ç­†è¨˜éŒ„")
                    else:
                        logger.debug(f"  - {table}: ç„¡æ–°è³‡æ–™")

                except Exception as e:
                    logger.error(f"  âœ— {table} åŒæ­¥å¤±æ•—: {e}")
                    # å–®ä¸€è¡¨æ ¼å¤±æ•—ä¸å½±éŸ¿å…¶ä»–è¡¨æ ¼
                    continue

        finally:
            await source_conn.close()
            await target_conn.close()

        return total_records

    async def sync_table(
        self,
        table: str,
        source_conn: asyncpg.Connection,
        target_conn: asyncpg.Connection
    ) -> int:
        """åŒæ­¥å–®ä¸€è¡¨æ ¼"""

        # 1. å–å¾—æœ€å¾ŒåŒæ­¥æ™‚é–“
        last_sync_time = self.state_manager.get_last_sync_time(table)

        # 2. å–å¾—å¢é‡è³‡æ–™
        data = await self.get_incremental_data(table, last_sync_time, source_conn)

        if not data:
            return 0

        # 3. å–å¾—éœ€è¦æ’é™¤çš„æ¬„ä½
        exclude_fields = SENSITIVE_FIELDS.get(table, [])

        # 4. UPSERT åˆ°ç›®æ¨™è³‡æ–™åº«
        count = await self.upsert_data(table, data, target_conn, exclude_fields)

        # 5. æ›´æ–°åŒæ­¥ç‹€æ…‹
        if count > 0:
            current_time = datetime.now()
            self.state_manager.update_sync_time(table, current_time, count)

        return count

    async def get_incremental_data(
        self,
        table: str,
        last_sync_time: Optional[datetime],
        conn: asyncpg.Connection
    ) -> List[Dict]:
        """å–å¾—å¢é‡è³‡æ–™"""

        # æª¢æŸ¥è¡¨æ ¼æ˜¯å¦æœ‰æ™‚é–“æˆ³è¨˜æ¬„ä½
        time_column = await self._get_time_column(table, conn)

        if not time_column:
            # ç„¡æ™‚é–“æˆ³è¨˜ï¼ŒåŸ·è¡Œå…¨é‡åŒæ­¥ï¼ˆä½†é »ç‡è¼ƒä½ï¼‰
            logger.debug(f"  è¡¨æ ¼ {table} ç„¡æ™‚é–“æˆ³è¨˜æ¬„ä½ï¼ŒåŸ·è¡Œå…¨é‡åŒæ­¥")
            query = f"SELECT * FROM {table}"
            rows = await conn.fetch(query)
            return [dict(row) for row in rows]

        # æ§‹å»ºå¢é‡æŸ¥è©¢
        if last_sync_time:
            query = f"""
                SELECT * FROM {table}
                WHERE {time_column} > $1
                ORDER BY {time_column}
            """
            rows = await conn.fetch(query, last_sync_time)
        else:
            # é¦–æ¬¡åŒæ­¥
            query = f"SELECT * FROM {table} ORDER BY {time_column}"
            rows = await conn.fetch(query)

        return [dict(row) for row in rows]

    async def _get_time_column(self, table: str, conn: asyncpg.Connection) -> Optional[str]:
        """æª¢æ¸¬è¡¨æ ¼çš„æ™‚é–“æˆ³è¨˜æ¬„ä½"""
        columns = await conn.fetch("""
            SELECT column_name
            FROM information_schema.columns
            WHERE table_name = $1
            AND column_name IN ('updated_at', 'created_at')
            ORDER BY CASE column_name
                WHEN 'updated_at' THEN 1
                WHEN 'created_at' THEN 2
            END
        """, table)

        if columns:
            return columns[0]['column_name']
        return None

    async def upsert_data(
        self,
        table: str,
        data: List[Dict],
        conn: asyncpg.Connection,
        exclude_fields: List[str] = None
    ) -> int:
        """æ‰¹æ¬¡ UPSERT è³‡æ–™åˆ°ç›®æ¨™è³‡æ–™åº«"""
        if not data:
            return 0

        exclude_fields = exclude_fields or []

        # å–å¾—ä¸»éµæ¬„ä½
        primary_key = await self._get_primary_key(table, conn)
        if not primary_key:
            logger.warning(f"  è¡¨æ ¼ {table} ç„¡ä¸»éµï¼Œè·³éåŒæ­¥")
            return 0

        # éæ¿¾æ¬„ä½
        filtered_data = []
        for row in data:
            filtered_row = {k: v for k, v in row.items() if k not in exclude_fields}
            filtered_data.append(filtered_row)

        if not filtered_data:
            return 0

        # æ‰¹æ¬¡è™•ç†
        total_upserted = 0
        batch_size = self.config.BATCH_SIZE

        for i in range(0, len(filtered_data), batch_size):
            batch = filtered_data[i:i + batch_size]
            count = await self._upsert_batch(table, batch, primary_key, conn)
            total_upserted += count

        return total_upserted

    async def _upsert_batch(
        self,
        table: str,
        batch: List[Dict],
        primary_key: str,
        conn: asyncpg.Connection
    ) -> int:
        """åŸ·è¡Œæ‰¹æ¬¡ UPSERT"""
        if not batch:
            return 0

        columns = list(batch[0].keys())
        column_names = ', '.join(columns)

        # æ§‹å»º VALUES å­å¥
        placeholders_list = []
        values_list = []

        for idx, row in enumerate(batch):
            row_placeholders = ', '.join([f'${len(values_list) + i + 1}' for i in range(len(columns))])
            placeholders_list.append(f'({row_placeholders})')
            values_list.extend([row[col] for col in columns])

        values_clause = ', '.join(placeholders_list)

        # æ§‹å»º ON CONFLICT UPDATE å­å¥
        update_clause = ', '.join([
            f"{col} = EXCLUDED.{col}"
            for col in columns if col != primary_key
        ])

        # æ§‹å»ºå®Œæ•´ SQL
        if update_clause:
            query = f"""
                INSERT INTO {table} ({column_names})
                VALUES {values_clause}
                ON CONFLICT ({primary_key})
                DO UPDATE SET {update_clause}
            """
        else:
            query = f"""
                INSERT INTO {table} ({column_names})
                VALUES {values_clause}
                ON CONFLICT ({primary_key}) DO NOTHING
            """

        # åŸ·è¡Œ UPSERT
        try:
            await conn.execute(query, *values_list)
            return len(batch)
        except Exception as e:
            logger.error(f"  UPSERT å¤±æ•—: {e}")
            raise

    async def _get_primary_key(self, table: str, conn: asyncpg.Connection) -> Optional[str]:
        """å–å¾—è¡¨æ ¼çš„ä¸»éµæ¬„ä½"""
        result = await conn.fetchrow("""
            SELECT a.attname
            FROM pg_index i
            JOIN pg_attribute a ON a.attrelid = i.indrelid AND a.attnum = ANY(i.indkey)
            WHERE i.indrelid = $1::regclass
            AND i.indisprimary
        """, table)

        if result:
            return result['attname']
        return None
