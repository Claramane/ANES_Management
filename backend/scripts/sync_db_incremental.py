#!/usr/bin/env python3
"""
PostgreSQL å¢é‡è³‡æ–™åŒæ­¥è…³æœ¬ (å‚™æ¡ˆæ–¹æ¡ˆ)

ç•¶ PostgreSQL é‚è¼¯è¤‡è£½ä¸å¯ç”¨æ™‚ï¼Œä½¿ç”¨æ­¤è…³æœ¬é€²è¡Œå¢é‡åŒæ­¥ã€‚

åŠŸèƒ½ï¼š
1. åŸºæ–¼ updated_at/created_at æ¬„ä½åˆ¤æ–·å¢é‡è³‡æ–™
2. æ™ºèƒ½è™•ç†å¤–éµä¾è³´é †åº
3. æ‰¹æ¬¡åŒæ­¥ä»¥æ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨
4. è¨˜éŒ„åŒæ­¥ç‹€æ…‹å’ŒéŒ¯èª¤æ—¥èªŒ
5. æ”¯æ´å¤šè³‡æ–™åº«ç›®æ¨™ï¼ˆæœ¬åœ°ã€æ¸¬è©¦ç’°å¢ƒï¼‰

ä½¿ç”¨æ–¹å¼ï¼š
    # åŒæ­¥åˆ°æœ¬åœ°è³‡æ–™åº«
    python sync_db_incremental.py --target local

    # åŒæ­¥åˆ°æ¸¬è©¦ç’°å¢ƒ
    python sync_db_incremental.py --target test

    # å¼·åˆ¶å…¨é‡åŒæ­¥
    python sync_db_incremental.py --target local --full

    # åªåŒæ­¥ç‰¹å®šè¡¨æ ¼
    python sync_db_incremental.py --target local --tables users,schedules

è¨­å®šæª”ï¼š
    sync_config.json - åŒ…å«è³‡æ–™åº«é€£ç·šå’ŒåŒæ­¥è¦å‰‡
"""

import sys
import os
from pathlib import Path

# æ·»åŠ  backend åˆ° Python è·¯å¾‘
backend_dir = Path(__file__).parent.parent
sys.path.insert(0, str(backend_dir))

import asyncio
import asyncpg
import json
import argparse
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
from sqlalchemy import create_engine, text, MetaData, Table
from sqlalchemy.orm import sessionmaker


# ============================================================================
# è¨­å®šèˆ‡å¸¸æ•¸
# ============================================================================

# è¡¨æ ¼åŒæ­¥é †åº (è€ƒæ…®å¤–éµä¾è³´)
TABLE_SYNC_ORDER = [
    'users',                    # æœ€å„ªå…ˆ (å…¶ä»–è¡¨æ ¼çš„å¤–éµåƒè€ƒ)
    'formula_schedules',        # ç­è¡¨å…¬å¼
    'schedules',                # ç­è¡¨ (ä¾è³´ users)
    'shift_swap_requests',      # èª¿ç­ç”³è«‹ (ä¾è³´ users, schedules)
    'overtime_records',         # åŠ ç­è¨˜éŒ„ (ä¾è³´ users, schedules)
    'announcements',            # å…¬å‘Š
    'doctor_schedules',         # é†«å¸«ç­è¡¨
    'doctor_schedule_details',  # é†«å¸«ç­è¡¨è©³æƒ… (ä¾è³´ doctor_schedules)
    'notifications',            # é€šçŸ¥ (ä¾è³´ users)
    'shift_swap_notifications', # èª¿ç­é€šçŸ¥ (ä¾è³´ shift_swap_requests, users)
    'webauthn_credentials',     # WebAuthn æ†‘è­‰ (ä¾è³´ users)
    'application_logs',         # æ‡‰ç”¨ç¨‹å¼æ—¥èªŒ (æœ€å¾Œ)
]

# éœ€è¦è„«æ•çš„æ¬„ä½
SENSITIVE_FIELDS = {
    'users': ['password'],  # å¯†ç¢¼æ¬„ä½ä¸åŒæ­¥
    'webauthn_credentials': ['credential_id', 'public_key'],  # ç”Ÿç‰©è¾¨è­˜è³‡æ–™ä¸åŒæ­¥
}

# æ‰¹æ¬¡å¤§å°
BATCH_SIZE = 1000


# ============================================================================
# åŒæ­¥ç‹€æ…‹ç®¡ç†
# ============================================================================

class SyncState:
    """åŒæ­¥ç‹€æ…‹ç®¡ç†"""

    def __init__(self, state_file: str = 'sync_state.json'):
        self.state_file = Path(__file__).parent / state_file
        self.state = self._load_state()

    def _load_state(self) -> Dict:
        """è¼‰å…¥åŒæ­¥ç‹€æ…‹"""
        if self.state_file.exists():
            with open(self.state_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}

    def save_state(self):
        """å„²å­˜åŒæ­¥ç‹€æ…‹"""
        with open(self.state_file, 'w', encoding='utf-8') as f:
            json.dump(self.state, f, indent=2, ensure_ascii=False, default=str)

    def get_last_sync_time(self, table: str) -> Optional[datetime]:
        """å–å¾—è¡¨æ ¼çš„æœ€å¾ŒåŒæ­¥æ™‚é–“"""
        if table in self.state:
            return datetime.fromisoformat(self.state[table]['last_sync_time'])
        return None

    def update_sync_time(self, table: str, sync_time: datetime, records_synced: int):
        """æ›´æ–°è¡¨æ ¼çš„åŒæ­¥æ™‚é–“"""
        self.state[table] = {
            'last_sync_time': sync_time.isoformat(),
            'records_synced': records_synced,
            'updated_at': datetime.now().isoformat()
        }
        self.save_state()

    def get_stats(self) -> Dict:
        """å–å¾—åŒæ­¥çµ±è¨ˆ"""
        return self.state


# ============================================================================
# è³‡æ–™åº«é€£ç·šç®¡ç†
# ============================================================================

class DatabaseConnector:
    """è³‡æ–™åº«é€£ç·šç®¡ç†å™¨"""

    def __init__(self, config_file: str = 'sync_config.json'):
        config_path = Path(__file__).parent / config_file
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)

    def get_source_url(self) -> str:
        """å–å¾—ä¾†æºè³‡æ–™åº«é€£ç·š"""
        return self.config['source']['database_url']

    def get_target_url(self, target: str = 'local') -> str:
        """å–å¾—ç›®æ¨™è³‡æ–™åº«é€£ç·š"""
        if target not in self.config['targets']:
            raise ValueError(f"Unknown target: {target}. Available: {list(self.config['targets'].keys())}")
        return self.config['targets'][target]['database_url']

    def get_sync_tables(self) -> List[str]:
        """å–å¾—éœ€è¦åŒæ­¥çš„è¡¨æ ¼æ¸…å–®"""
        return self.config.get('sync_tables', TABLE_SYNC_ORDER)

    def get_exclude_fields(self, table: str) -> List[str]:
        """å–å¾—éœ€è¦æ’é™¤çš„æ¬„ä½"""
        return self.config.get('exclude_fields', {}).get(table, [])


# ============================================================================
# è³‡æ–™åŒæ­¥é‚è¼¯
# ============================================================================

class IncrementalSync:
    """å¢é‡è³‡æ–™åŒæ­¥"""

    def __init__(self, source_url: str, target_url: str):
        self.source_url = source_url
        self.target_url = target_url
        self.sync_state = SyncState()

    async def get_incremental_data(
        self,
        table: str,
        last_sync_time: Optional[datetime],
        conn: asyncpg.Connection
    ) -> List[Dict]:
        """å–å¾—å¢é‡è³‡æ–™"""

        # æª¢æŸ¥è¡¨æ ¼æ˜¯å¦æœ‰æ™‚é–“æˆ³è¨˜æ¬„ä½
        time_column = await self._get_time_column(table, conn)
        if not time_column:
            print(f"   âš ï¸  è¡¨æ ¼ {table} ç„¡æ™‚é–“æˆ³è¨˜æ¬„ä½ï¼ŒåŸ·è¡Œå…¨é‡åŒæ­¥")
            query = f"SELECT * FROM {table}"
            rows = await conn.fetch(query)
            return [dict(row) for row in rows]

        # æ§‹å»ºå¢é‡æŸ¥è©¢
        if last_sync_time:
            query = f"""
                SELECT * FROM {table}
                WHERE {time_column} > $1
                ORDER BY {time_column}
            """
            rows = await conn.fetch(query, last_sync_time)
        else:
            query = f"SELECT * FROM {table} ORDER BY {time_column}"
            rows = await conn.fetch(query)

        return [dict(row) for row in rows]

    async def _get_time_column(self, table: str, conn: asyncpg.Connection) -> Optional[str]:
        """æª¢æ¸¬è¡¨æ ¼çš„æ™‚é–“æˆ³è¨˜æ¬„ä½"""
        columns = await conn.fetch("""
            SELECT column_name
            FROM information_schema.columns
            WHERE table_name = $1
            AND column_name IN ('updated_at', 'created_at')
            ORDER BY CASE column_name
                WHEN 'updated_at' THEN 1
                WHEN 'created_at' THEN 2
            END
        """, table)

        if columns:
            return columns[0]['column_name']
        return None

    async def upsert_data(
        self,
        table: str,
        data: List[Dict],
        conn: asyncpg.Connection,
        exclude_fields: List[str] = None
    ) -> int:
        """æ‰¹æ¬¡ upsert è³‡æ–™åˆ°ç›®æ¨™è³‡æ–™åº«"""
        if not data:
            return 0

        exclude_fields = exclude_fields or []

        # å–å¾—ä¸»éµæ¬„ä½
        primary_key = await self._get_primary_key(table, conn)
        if not primary_key:
            print(f"   âš ï¸  è¡¨æ ¼ {table} ç„¡ä¸»éµï¼Œè·³éåŒæ­¥")
            return 0

        # éæ¿¾æ¬„ä½
        filtered_data = []
        for row in data:
            filtered_row = {k: v for k, v in row.items() if k not in exclude_fields}
            filtered_data.append(filtered_row)

        # æ§‹å»º upsert æŸ¥è©¢
        if not filtered_data:
            return 0

        columns = list(filtered_data[0].keys())
        placeholders = ', '.join([f'${i+1}' for i in range(len(columns))])
        column_names = ', '.join(columns)

        # æ§‹å»º ON CONFLICT UPDATE å­å¥
        update_clause = ', '.join([
            f"{col} = EXCLUDED.{col}"
            for col in columns if col != primary_key
        ])

        upsert_query = f"""
            INSERT INTO {table} ({column_names})
            VALUES ({placeholders})
            ON CONFLICT ({primary_key})
            DO UPDATE SET {update_clause}
        """

        # æ‰¹æ¬¡åŸ·è¡Œ
        synced_count = 0
        for i in range(0, len(filtered_data), BATCH_SIZE):
            batch = filtered_data[i:i + BATCH_SIZE]
            async with conn.transaction():
                for row in batch:
                    values = [row[col] for col in columns]
                    await conn.execute(upsert_query, *values)
                    synced_count += 1

        return synced_count

    async def _get_primary_key(self, table: str, conn: asyncpg.Connection) -> Optional[str]:
        """å–å¾—è¡¨æ ¼çš„ä¸»éµæ¬„ä½"""
        result = await conn.fetch("""
            SELECT a.attname
            FROM pg_index i
            JOIN pg_attribute a ON a.attrelid = i.indrelid AND a.attnum = ANY(i.indkey)
            WHERE i.indrelid = $1::regclass AND i.indisprimary
        """, table)

        if result:
            return result[0]['attname']
        return None

    async def sync_table(
        self,
        table: str,
        force_full: bool = False,
        exclude_fields: List[str] = None
    ) -> Tuple[int, datetime]:
        """åŒæ­¥å–®ä¸€è¡¨æ ¼"""

        # é€£æ¥è³‡æ–™åº«
        source_conn = await asyncpg.connect(self.source_url)
        target_conn = await asyncpg.connect(self.target_url)

        try:
            # å–å¾—æœ€å¾ŒåŒæ­¥æ™‚é–“
            last_sync_time = None if force_full else self.sync_state.get_last_sync_time(table)

            # å–å¾—å¢é‡è³‡æ–™
            print(f"   ğŸ“¥ å¾ä¾†æºè³‡æ–™åº«è®€å–å¢é‡è³‡æ–™...")
            data = await self.get_incremental_data(table, last_sync_time, source_conn)

            if not data:
                print(f"   â„¹ï¸  ç„¡æ–°è³‡æ–™éœ€è¦åŒæ­¥")
                return 0, datetime.now()

            print(f"   ğŸ“¦ å–å¾— {len(data)} ç­†è¨˜éŒ„")

            # å¯«å…¥ç›®æ¨™è³‡æ–™åº«
            print(f"   ğŸ“¤ å¯«å…¥ç›®æ¨™è³‡æ–™åº«...")
            synced_count = await self.upsert_data(table, data, target_conn, exclude_fields)

            sync_time = datetime.now()
            print(f"   âœ… æˆåŠŸåŒæ­¥ {synced_count} ç­†è¨˜éŒ„")

            return synced_count, sync_time

        finally:
            await source_conn.close()
            await target_conn.close()

    async def sync_all_tables(
        self,
        tables: List[str],
        force_full: bool = False,
        connector: DatabaseConnector = None
    ):
        """åŒæ­¥æ‰€æœ‰è¡¨æ ¼"""
        print(f"\n{'='*70}")
        print(f"ğŸ”„ é–‹å§‹å¢é‡è³‡æ–™åŒæ­¥")
        print(f"{'='*70}")
        print(f"ä¾†æº: {self.source_url.split('@')[1] if '@' in self.source_url else 'local'}")
        print(f"ç›®æ¨™: {self.target_url.split('@')[1] if '@' in self.target_url else 'local'}")
        print(f"æ¨¡å¼: {'å…¨é‡åŒæ­¥' if force_full else 'å¢é‡åŒæ­¥'}")
        print(f"{'='*70}\n")

        total_synced = 0
        start_time = datetime.now()

        for table in tables:
            print(f"ğŸ“‹ åŒæ­¥è¡¨æ ¼: {table}")

            # å–å¾—æ’é™¤æ¬„ä½
            exclude_fields = connector.get_exclude_fields(table) if connector else []
            if table in SENSITIVE_FIELDS:
                exclude_fields.extend(SENSITIVE_FIELDS[table])

            if exclude_fields:
                print(f"   ğŸ”’ æ’é™¤æ¬„ä½: {', '.join(exclude_fields)}")

            try:
                synced_count, sync_time = await self.sync_table(
                    table,
                    force_full=force_full,
                    exclude_fields=exclude_fields
                )

                # æ›´æ–°åŒæ­¥ç‹€æ…‹
                self.sync_state.update_sync_time(table, sync_time, synced_count)
                total_synced += synced_count

            except Exception as e:
                print(f"   âŒ åŒæ­¥å¤±æ•—: {e}")
                continue

            print()

        # ç¸½çµ
        elapsed_time = (datetime.now() - start_time).total_seconds()
        print(f"{'='*70}")
        print(f"âœ… åŒæ­¥å®Œæˆ")
        print(f"{'='*70}")
        print(f"ç¸½å…±åŒæ­¥: {total_synced} ç­†è¨˜éŒ„")
        print(f"è€—æ™‚: {elapsed_time:.2f} ç§’")
        print(f"{'='*70}\n")


# ============================================================================
# å‘½ä»¤åˆ—ä»‹é¢
# ============================================================================

async def main():
    parser = argparse.ArgumentParser(description='PostgreSQL å¢é‡è³‡æ–™åŒæ­¥å·¥å…·')
    parser.add_argument(
        '--target',
        choices=['local', 'test'],
        default='local',
        help='ç›®æ¨™ç’°å¢ƒ (local: æœ¬åœ°, test: æ¸¬è©¦ç’°å¢ƒ)'
    )
    parser.add_argument(
        '--full',
        action='store_true',
        help='å¼·åˆ¶å…¨é‡åŒæ­¥ (å¿½ç•¥ä¸Šæ¬¡åŒæ­¥æ™‚é–“)'
    )
    parser.add_argument(
        '--tables',
        type=str,
        help='æŒ‡å®šè¦åŒæ­¥çš„è¡¨æ ¼ï¼Œç”¨é€—è™Ÿåˆ†éš” (é è¨­: åŒæ­¥æ‰€æœ‰è¡¨æ ¼)'
    )
    parser.add_argument(
        '--config',
        type=str,
        default='sync_config.json',
        help='é…ç½®æ–‡ä»¶è·¯å¾‘ (é è¨­: sync_config.json)'
    )

    args = parser.parse_args()

    # è¼‰å…¥é…ç½®
    connector = DatabaseConnector(args.config)

    # å–å¾—è³‡æ–™åº«é€£ç·š
    source_url = connector.get_source_url()
    target_url = connector.get_target_url(args.target)

    # å–å¾—è¦åŒæ­¥çš„è¡¨æ ¼
    if args.tables:
        tables = [t.strip() for t in args.tables.split(',')]
    else:
        tables = connector.get_sync_tables()

    # åŸ·è¡ŒåŒæ­¥
    sync = IncrementalSync(source_url, target_url)
    await sync.sync_all_tables(tables, force_full=args.full, connector=connector)


if __name__ == "__main__":
    asyncio.run(main())
